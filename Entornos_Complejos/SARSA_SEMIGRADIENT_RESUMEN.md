# Resumen: ImplementaciÃ³n del Agente SARSA Semi-Gradiente

## ğŸ“‹ Archivos Creados

### 1. **Agente Principal**
- **Ruta:** `src/agents/sarsa_semigradient.py`
- **Clase:** `AgentSarsaSemiGradient`
- **DescripciÃ³n:** ImplementaciÃ³n completa del algoritmo SARSA con aproximaciÃ³n de funciones lineales

### 2. **Archivo de Ejemplos**
- **Ruta:** `examples/ejemplo_sarsa_semigradient.py`
- **Contenido:**
  - 4 extractores de caracterÃ­sticas diferentes:
    - Tile Coding
    - CaracterÃ­sticas PolinÃ³micas
    - Funciones de Base Radial (RBF)
    - CaracterÃ­sticas Simples
  - FunciÃ³n de entrenamiento completa
  - Ejemplo ejecutable

### 3. **DocumentaciÃ³n**
- **Ruta:** `docs/SARSA_SEMIGRADIENT.md`
- **Contenido:**
  - ExplicaciÃ³n teÃ³rica del algoritmo
  - Fundamento matemÃ¡tico
  - ComparaciÃ³n con SARSA tabular
  - GuÃ­a de uso completa
  - Consejos prÃ¡cticos
  - Referencias

### 4. **Tests**
- **Ruta:** `tests/test_sarsa_semigradient.py`
- **Tests incluidos:**
  - InicializaciÃ³n del agente
  - SelecciÃ³n de acciones
  - ActualizaciÃ³n de pesos
  - CÃ¡lculo de valores Q
  - Entrenamiento de episodios
  - GestiÃ³n de pesos

### 5. **Archivos Actualizados**
- `src/agents/__init__.py` - AÃ±adido `AgentSarsaSemiGradient` a las exportaciones
- `src/policies/__init__.py` - AÃ±adidas las polÃ­ticas a las exportaciones

---

## ğŸ¯ CaracterÃ­sticas Principales del Agente

### ParÃ¡metros del Constructor

```python
    AgentSarsaSemiGradient(
        env: Env,                    # Entorno de Gymnasium
        policy: Policy,              # PolÃ­tica (on-policy)
        feature_extractor: callable, # FunciÃ³n de extracciÃ³n de caracterÃ­sticas
        n_features: int,             # NÃºmero de caracterÃ­sticas
        alpha: float = 0.01,         # Tasa de aprendizaje
        gamma: float = 0.99          # Factor de descuento
    )
```

### MÃ©todos Principales

1. **`get_action(state)`** - Selecciona una acciÃ³n segÃºn la polÃ­tica
2. **`update(state, action, reward, next_state, done)`** - Actualiza los pesos usando SARSA semi-gradiente
3. **`get_weights()`** - Obtiene una copia del vector de pesos
4. **`set_weights(weights)`** - Establece el vector de pesos
5. **`reset_weights()`** - Reinicia los pesos a cero

### MÃ©todos Internos

- **`_get_features(state, action)`** - Extrae caracterÃ­sticas del par estado-acciÃ³n
- **`_get_q_value(state, action)`** - Calcula qÌ‚(s, a) = w^T Â· Ï†(s, a)
- **`_get_all_q_values(state)`** - Calcula valores Q para todas las acciones

---

## ğŸ”¬ Algoritmo Implementado

### FÃ³rmula de ActualizaciÃ³n

```
w â† w + Î± Â· [R + Î³ Â· qÌ‚(S', A', w) - qÌ‚(S, A, w)] Â· Ï†(S, A)
```

Donde:
- **w**: Vector de pesos
- **Î±**: Tasa de aprendizaje (alpha)
- **R**: Recompensa recibida
- **Î³**: Factor de descuento (gamma)
- **qÌ‚(S, A, w)**: AproximaciÃ³n del valor Q = w^T Â· Ï†(S, A)
- **Ï†(S, A)**: Vector de caracterÃ­sticas

### Flujo del Algoritmo

1. Inicializar pesos w arbitrariamente
2. Para cada episodio:
   - Inicializar S
   - Elegir A usando la polÃ­tica derivada de qÌ‚
   - Para cada paso del episodio:
     - Tomar acciÃ³n A, observar R, S'
     - Elegir A' usando la polÃ­tica derivada de qÌ‚
     - Actualizar: w â† w + Î± Â· [R + Î³ Â· qÌ‚(S', A', w) - qÌ‚(S, A, w)] Â· Ï†(S, A)
     - S â† S', A â† A'
   - Hasta que S sea terminal

---

## ğŸ“Š Extractores de CaracterÃ­sticas Incluidos

### 1. Tile Coding
- **Uso:** Espacios continuos multidimensionales
- **Ventajas:** Buena generalizaciÃ³n local, eficiente
- **ParÃ¡metros:** `n_tilings`, `n_tiles_per_dim`

### 2. CaracterÃ­sticas PolinÃ³micas
- **Uso:** Aproximar funciones no lineales
- **Ventajas:** Captura interacciones entre variables
- **ParÃ¡metros:** `degree` (grado del polinomio)

### 3. Funciones de Base Radial (RBF)
- **Uso:** Funciones suaves, similitudes locales
- **Ventajas:** AproximaciÃ³n suave, interpretable
- **ParÃ¡metros:** `n_centers`, `sigma`

### 4. CaracterÃ­sticas Simples
- **Uso:** Baseline, espacios pequeÃ±os
- **Ventajas:** Simple, rÃ¡pido
- **ParÃ¡metros:** Ninguno

---

## ğŸš€ Ejemplo de Uso RÃ¡pido

```python
import gymnasium as gym
from Entornos_Complejos.src.agents import AgentSarsaSemiGradient
from Entornos_Complejos.src.policies import EpsilonGreedyPolicy
import numpy as np

# Crear entorno
env = gym.make('CartPole-v1')

# Definir extractor de caracterÃ­sticas
def feature_extractor(state, action, env):
    state = np.array(state)
    n_actions = env.action_space.n
    base_features = np.concatenate([[1.0], state])
    features = np.zeros(len(base_features) * n_actions)
    features[action * len(base_features):(action + 1) * len(base_features)] = base_features
    return features

# Crear polÃ­tica
policy = EpsilonGreedyPolicy(epsilon=0.1, n_actions=env.action_space.n)

# Crear agente
agent = AgentSarsaSemiGradient(
    env=env,
    policy=policy,
    feature_extractor=feature_extractor,
    n_features=10,  # (1 + 4 dimensiones) * 2 acciones
    alpha=0.01,
    gamma=0.99
)

# Entrenar
for episode in range(500):
    state, _ = env.reset()
    action = agent.get_action(state)
    done = False

    while not done:
        next_state, reward, done, truncated, _ = env.step(action)
        next_action = agent.update(state, action, reward, next_state, done or truncated)
        state = next_state
        action = next_action if next_action is not None else agent.get_action(state)
```

---

## âœ… Tests Disponibles

Para ejecutar los tests:

```bash
python Entornos_Complejos/tests/test_sarsa_semigradient.py
```

Tests incluidos:
- âœ“ InicializaciÃ³n del agente
- âœ“ SelecciÃ³n de acciones vÃ¡lidas
- âœ“ ActualizaciÃ³n de pesos
- âœ“ CÃ¡lculo de valores Q
- âœ“ Entrenamiento de episodios completos
- âœ“ GestiÃ³n de pesos (get/set/reset)

---

## ğŸ“š Diferencias Clave con Otros Agentes

### vs. SARSA Tabular (`AgentSarsa`)
- **SARSA Tabular:** Usa tabla Q discreta, solo espacios discretos pequeÃ±os
- **SARSA Semi-Gradiente:** Usa aproximaciÃ³n lineal, espacios continuos o grandes

### vs. Q-Learning (`AgentQLearning`)
- **Q-Learning:** Off-policy, actualiza hacia el mÃ¡ximo Q
- **SARSA Semi-Gradiente:** On-policy, actualiza hacia la acciÃ³n seleccionada

### vs. Monte Carlo (`AgentMonteCarlo`)
- **Monte Carlo:** Aprende al final del episodio, sin bootstrapping
- **SARSA Semi-Gradiente:** Aprende en cada paso, con bootstrapping

---

## ğŸ“ CuÃ¡ndo Usar SARSA Semi-Gradiente

### âœ… Usar cuando:
- El espacio de estados es continuo (ej: CartPole, MountainCar)
- El espacio de estados es muy grande
- Necesitas generalizaciÃ³n entre estados similares
- Quieres un mÃ©todo on-policy (mÃ¡s estable que off-policy)

### âŒ No usar cuando:
- El espacio de estados es pequeÃ±o y discreto (usa SARSA tabular)
- Necesitas garantÃ­as de convergencia
- La funciÃ³n Q es muy compleja (considera redes neuronales/DQN)

---

## ğŸ“– Referencias y Recursos

- **Libro:** Sutton & Barto (2018) - "Reinforcement Learning: An Introduction"
  - CapÃ­tulo 9: On-policy Prediction with Approximation
  - CapÃ­tulo 10: On-policy Control with Approximation

- **DocumentaciÃ³n completa:** `docs/SARSA_SEMIGRADIENT.md`
- **Ejemplos:** `examples/ejemplo_sarsa_semigradient.py`

---

## ğŸ”§ PrÃ³ximos Pasos Sugeridos

1. **Experimentar con diferentes extractores de caracterÃ­sticas**
   - Probar tile coding con diferentes configuraciones
   - Ajustar el grado de las caracterÃ­sticas polinÃ³micas

2. **Ajustar hiperparÃ¡metros**
   - Probar diferentes valores de alpha (0.0001 - 0.1)
   - Experimentar con diferentes valores de epsilon

3. **Probar en diferentes entornos**
   - MountainCar-v0 (continuo)
   - Acrobot-v1
   - LunarLander-v2

4. **Implementar mejoras**
   - Alpha decreciente (learning rate decay)
   - NormalizaciÃ³n de caracterÃ­sticas
   - Eligibility traces (SARSA(Î»))

---

## ğŸ“ Notas Importantes

- El agente es **on-policy**, lo que significa que aprende sobre la misma polÃ­tica que usa para actuar
- El mÃ©todo es **semi-gradiente** porque no calcula el gradiente completo (trata el target como constante)
- La **elecciÃ³n del extractor de caracterÃ­sticas** es crucial para el rendimiento
- Requiere **ajuste cuidadoso de alpha** para evitar divergencia
- **No garantiza convergencia** como los mÃ©todos tabulares

---

## ğŸ‰ Resumen

Se ha implementado exitosamente el agente **SARSA Semi-Gradiente** como una nueva subclase de `Agent`, incluyendo:

- âœ… ImplementaciÃ³n completa del algoritmo
- âœ… 4 extractores de caracterÃ­sticas diferentes
- âœ… DocumentaciÃ³n detallada
- âœ… Ejemplos de uso
- âœ… Suite de tests
- âœ… IntegraciÃ³n con el sistema de polÃ­ticas existente

El agente estÃ¡ listo para ser usado en problemas de aprendizaje por refuerzo con espacios de estados continuos o muy grandes.
