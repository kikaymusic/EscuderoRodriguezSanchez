{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f3e9abd1362f6f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ldaniel-hm/eml_k_bandit/blob/main/bandit_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1697e197fa5a08",
   "metadata": {
    "id": "7c1697e197fa5a08"
   },
   "source": [
    "## Preparaci칩n del entorno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c661e3bdc284fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T20:19:07.667022Z",
     "start_time": "2026-02-09T20:19:07.661345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/kikay/Documents/Master/EscuderoRodriguezSanchez/k_brazos\n",
      "Added to path: /Users/kikay/Documents/Master/EscuderoRodriguezSanchez/k_brazos\n"
     ]
    }
   ],
   "source": [
    "# Configurar el path para imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# A침adir el directorio k_brazos al path de Python\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__')) if '__file__' in globals() else os.getcwd()\n",
    "k_brazos_dir = notebook_dir if 'k_brazos' in notebook_dir else os.path.join(notebook_dir, 'k_brazos')\n",
    "if k_brazos_dir not in sys.path:\n",
    "    sys.path.insert(0, k_brazos_dir)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Added to path: {k_brazos_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4582eec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T20:19:09.939256Z",
     "start_time": "2026-02-09T20:19:08.948667Z"
    },
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "4582eec6",
    "outputId": "e7fac2c4-73e6-4271-a94a-4f40c920d478"
   },
   "outputs": [],
   "source": [
    "#@title Importamos todas las clases y funciones\n",
    "from src.algorithms.algorithm import Algorithm\n",
    "from src.algorithms.epsilon_greedy import EpsilonGreedy\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from src.algorithms.softmax import Softmax\n",
    "#from src.algorithms.softmax import Softmax\n",
    "from src.arms import ArmNormal, Bandit\n",
    "from src.plotting import plot_average_rewards, plot_optimal_selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e67de1a19a3698f",
   "metadata": {
    "id": "4e67de1a19a3698f"
   },
   "source": [
    "## Experimento\n",
    "\n",
    "Cada algoritmo se ejecuta en un problema de k-armed bandit durante un n칰mero de pasos de tiempo y ejecuciones determinado.\n",
    "Se comparan los resultados de los algoritmos en t칠rminos de recompensa promedio.\n",
    "\n",
    "Por ejemplo. Dado un bandido de k-brazos, se ejecutan dos algoritmos epsilon-greedy con diferentes valores de epsilon. Se estudia la evoluci칩n de cada pol칤tica  en un n칰mero de pasos, por ejemplo, mil pasos. Entonces se repite el experimento un n칰mero de veces, por ejemplo, 500 veces. Es decir, se ejecutan 500 veces la evoluci칩n de cada algoritmo en 1000 pasos. Para cada paso calculamos el promedio de las recoponensas obtenidas en esas 500 veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7377ca48ee0f5946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T20:19:18.473175Z",
     "start_time": "2026-02-09T20:19:18.461233Z"
    },
    "id": "7377ca48ee0f5946"
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
    "\n",
    "    optimal_arm = bandit.optimal_arm  # Necesario para calcular el porcentaje de selecciones 칩ptimas.\n",
    "\n",
    "    rewards = np.zeros((len(algorithms), steps)) # Matriz para almacenar las recompensas promedio.\n",
    "\n",
    "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para almacenar el porcentaje de selecciones 칩ptimas.\n",
    "\n",
    "    np.random.seed(seed)  # Asegurar reproducibilidad de resultados.\n",
    "\n",
    "    for run in range(runs):\n",
    "        current_bandit = Bandit(arms=bandit.arms)\n",
    "\n",
    "        for algo in algorithms:\n",
    "            algo.reset() # Reiniciar los valores de los algoritmos.\n",
    "\n",
    "        total_rewards_per_algo = np.zeros(len(algorithms)) # Acumulador de recompensas por algoritmo. Necesario para calcular el promedio.\n",
    "\n",
    "        for step in range(steps):\n",
    "            for idx, algo in enumerate(algorithms):\n",
    "                chosen_arm = algo.select_arm() # Seleccionar un brazo seg칰n la pol칤tica del algoritmo.\n",
    "                reward = current_bandit.pull_arm(chosen_arm) # Obtener la recompensa del brazo seleccionado.\n",
    "                algo.update(chosen_arm, reward) # Actualizar el valor estimado del brazo seleccionado.\n",
    "\n",
    "                rewards[idx, step] += reward # Acumular la recompensa obtenida en la matriz rewards para el algoritmo idx en el paso step.\n",
    "                total_rewards_per_algo[idx] += reward # Acumular la recompensa obtenida en total_rewards_per_algo para el algoritmo idx.\n",
    "\n",
    "                #TODO: modificar optimal_selections cuando el brazo elegido se corresponda con el brazo 칩ptimo optimal_arm\n",
    "\n",
    "\n",
    "    rewards /= runs\n",
    "\n",
    "    # TODO: calcular el porcentaje de selecciones 칩ptimas y almacenar en optimal_selections\n",
    "\n",
    "    return rewards, optimal_selections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5453755a2b2a8",
   "metadata": {
    "id": "3fb5453755a2b2a8"
   },
   "source": [
    "## Ejecuci칩n del experimento\n",
    "\n",
    "Se realiza el experimento usando 10 brazos, cada uno de acuerdo a una distribuci칩n gaussina con desviaci칩n 1. Se realizan 500 ejecuciones de 1000 pasos cada una. Se contrastan 3 algoritmos epsilon greedy para valores epsilon: 0.0, 0.01, y 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "157bf5cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T20:10:28.822752500Z",
     "start_time": "2025-01-29T15:16:59.172459Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "157bf5cc",
    "outputId": "7e8aa027-53e5-4851-a800-4ee1387917c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit with 10 arms: ArmNormal(mu=1.52, sigma=1.0), ArmNormal(mu=2.4, sigma=1.0), ArmNormal(mu=1.19, sigma=1.0), ArmNormal(mu=4.37, sigma=1.0), ArmNormal(mu=6.39, sigma=1.0), ArmNormal(mu=7.59, sigma=1.0), ArmNormal(mu=8.8, sigma=1.0), ArmNormal(mu=9.56, sigma=1.0), ArmNormal(mu=6.41, sigma=1.0), ArmNormal(mu=7.37, sigma=1.0)\n",
      "Optimal arm: 8 with expected reward=9.56\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Par치metros del experimento\n",
    "seed = 42\n",
    "np.random.seed(seed)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "k = 10  # N칰mero de brazos\n",
    "steps = 1000  # N칰mero de pasos que se ejecutar치n cada algoritmo\n",
    "runs = 500  # N칰mero de ejecuciones\n",
    "\n",
    "# Creaci칩n del bandit\n",
    "bandit = Bandit(arms=ArmNormal.generate_arms(k)) # Generar un bandido con k brazos de distribuci칩n normal\n",
    "print(bandit)\n",
    "\n",
    "optimal_arm = bandit.optimal_arm\n",
    "print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
    "\n",
    "# Definir los algoritmos a comparar. En este caso son 3 algoritmos epsilon-greedy con diferentes valores de epsilon.\n",
    "#algorithms = [EpsilonGreedy(k=k, epsilon=0), EpsilonGreedy(k=k, epsilon=0.01), EpsilonGreedy(k=k, epsilon=0.1)]\n",
    "algorithms = [\n",
    "    Softmax(k=k, tau=0.1),  # Casi Greedy (Explotaci칩n)\n",
    "    Softmax(k=k, tau=1.0),  # Balanceado\n",
    "    Softmax(k=k, tau=10.0)  # Muy aleatorio (Exploraci칩n)\n",
    "]\n",
    "# Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones 칩ptimas\n",
    "rewards, optimal_selections = run_experiment(bandit, algorithms, steps, runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c9612c3b14efb",
   "metadata": {
    "id": "573c9612c3b14efb"
   },
   "source": [
    "## Visualizaci칩n de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fd68ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T20:10:28.845557200Z",
     "start_time": "2025-01-29T15:17:07.603719Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "d3fd68ee",
    "outputId": "d119bde5-5f0b-48ae-92f9-9850f69ccf07"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "El algoritmo debe ser de la clase Algorithm o una subclase.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Graficar los resultados\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplot_average_rewards\u001b[49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# plot_optimal_selections(steps, optimal_selections, algorithms)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master/EscuderoRodriguezSanchez/k_brazos/src/plotting/plotting.py:57\u001b[39m, in \u001b[36mplot_average_rewards\u001b[39m\u001b[34m(steps, rewards, algorithms)\u001b[39m\n\u001b[32m     55\u001b[39m plt.figure(figsize=(\u001b[32m14\u001b[39m, \u001b[32m7\u001b[39m))\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, algo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(algorithms):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     label = \u001b[43mget_algorithm_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     plt.plot(\u001b[38;5;28mrange\u001b[39m(steps), rewards[idx], label=label, linewidth=\u001b[32m2\u001b[39m)\n\u001b[32m     60\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mPasos de Tiempo\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m14\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Master/EscuderoRodriguezSanchez/k_brazos/src/plotting/plotting.py:41\u001b[39m, in \u001b[36mget_algorithm_label\u001b[39m\u001b[34m(algo)\u001b[39m\n\u001b[32m     36\u001b[39m     label += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m (epsilon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo.epsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# elif isinstance(algo, OtroAlgoritmo):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m#     label += f\" (parametro={algo.parametro})\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# A침adir m치s condiciones para otros algoritmos aqu칤\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEl algoritmo debe ser de la clase Algorithm o una subclase.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m label\n",
      "\u001b[31mValueError\u001b[39m: El algoritmo debe ser de la clase Algorithm o una subclase."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Graficar los resultados\n",
    "plot_average_rewards(steps, rewards, algorithms)\n",
    "# plot_optimal_selections(steps, optimal_selections, algorithms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcd95b7fca79de",
   "metadata": {
    "id": "29fcd95b7fca79de"
   },
   "source": [
    "### An치lisis detallado de la imagen\n",
    "\n",
    "La imagen muestra un gr치fico de l칤neas titulado **\"Recompensa Promedio vs Pasos de Tiempo\"**, donde se analiza el desempe침o de diferentes estrategias del algoritmo **풧-Greedy** en un entorno de multi-armed bandit. En el eje **x** se representan los **pasos de tiempo**, mientras que en el eje **y** se muestra la **recompensa promedio** obtenida por cada algoritmo.\n",
    "\n",
    "\n",
    "1. **Tres l칤neas de colores distintos representan diferentes valores de 풧 en el algoritmo 풧-Greedy:**\n",
    "   - **Azul (풧 = 0):** Representa una estrategia completamente **explotadora**, es decir, que siempre elige la acci칩n que ha dado la mejor recompensa hasta ahora sin explorar nuevas opciones.\n",
    "   - **Naranja (풧 = 0.01):** Representa una estrategia con una peque침a probabilidad del 1% de elegir una acci칩n aleatoria (exploraci칩n).\n",
    "   - **Verde (풧 = 0.1):** Representa una estrategia con un 10% de probabilidad de explorar acciones aleatorias.\n",
    "\n",
    "2. **Crecimiento de la recompensa promedio:**\n",
    "   - La l칤nea **verde (풧=0.1)** alcanza r치pidamente una recompensa promedio alta, lo que indica que la estrategia con mayor exploraci칩n aprende m치s r치pido qu칠 brazos del bandit son 칩ptimos.\n",
    "   - La l칤nea **naranja (풧=0.01)** tambi칠n muestra un crecimiento, pero m치s lento en comparaci칩n con 풧=0.1.\n",
    "   - La l칤nea **azul (풧=0)** se mantiene en un nivel bajo de recompensa, lo que sugiere que no logra encontrar el mejor brazo porque no explora nuevas opciones.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207366e56a23449",
   "metadata": {
    "id": "4207366e56a23449"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "Hemos estudiado un  **experimento de toma de decisiones secuenciales**, modelado con un **Multi-Armed Bandit (MAB)**. Este problema es fundamental en el aprendizaje por refuerzo y la teor칤a de decisiones. La idea principal es que un agente debe aprender cu치l es la mejor acci칩n (brazo del bandit) a partir de la experiencia acumulada. Para este estudio nos hemos centrado solo en el estudio del algoritmo epsilon-greedy, llegando a las siguientes conclusiones a partir de los resultados obtenidos y la gr치fica generada:\n",
    "\n",
    "#### **1. Exploraci칩n vs Explotaci칩n**\n",
    "El algoritmo **풧-Greedy** equilibra la exploraci칩n y la explotaci칩n:\n",
    "- **Explotaci칩n (풧=0)**: Siempre elige la mejor opci칩n conocida, pero si inicialmente se selecciona un brazo sub칩ptimo, nunca descubrir치 otras opciones m치s rentables.\n",
    "- **Exploraci칩n (풧>0)**: Introduce aleatoriedad en la selecci칩n de acciones para descubrir nuevas opciones potencialmente mejores.\n",
    "\n",
    "El gr치fico confirma este comportamiento:\n",
    "- **풧=0.1 (verde)** obtiene la mejor recompensa promedio a lo largo del tiempo porque explora lo suficiente como para encontrar r치pidamente el mejor brazo.\n",
    "- **풧=0.01 (naranja)** explora menos, por lo que tarda m치s en converger a una recompensa alta.\n",
    "- **풧=0 (azul)** no explora en absoluto y queda atrapado en una recompensa sub칩ptima.\n",
    "\n",
    "#### **2. Convergencia de los algoritmos**\n",
    "Los algoritmos con mayor exploraci칩n (풧=0.1) alcanzan una recompensa alta m치s r치pido. Esto se debe a que:\n",
    "- Al principio, el algoritmo **no tiene informaci칩n suficiente** sobre cu치l es el mejor brazo.\n",
    "- Con el tiempo, al realizar exploraciones, descubre cu치l es el mejor brazo y empieza a explotarlo m치s.\n",
    "- Un **balance entre exploraci칩n y explotaci칩n** es clave para maximizar la recompensa a largo plazo.\n",
    "\n",
    "\n",
    "#### **3. Aplicaciones y conclusiones**\n",
    "- En problemas de toma de decisiones **(ejemplo: recomendaciones, optimizaci칩n de anuncios, medicina personalizada)**, una estrategia de exploraci칩n moderada como **풧=0.1** es m치s efectiva para encontrar la mejor opci칩n r치pidamente.\n",
    "- **La falta de exploraci칩n (풧=0)** lleva a un desempe침o deficiente, ya que el agente puede quedarse atrapado en una elecci칩n sub칩ptima.\n",
    "\n",
    "En conclusi칩n, **el gr치fico muestra c칩mo un nivel adecuado de exploraci칩n mejora significativamente el rendimiento del algoritmo en un entorno de aprendizaje por refuerzo**. 游"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "extensiones",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
